Libreswan cluster on Oracle OCI infrastructure
----
Libreswan is one of recommended methods to connect two Cloud giants: Oracle OCI, and Amazon AWS. The problem is that both assumes that connection will be done to on premise equipment (CPE as Oracle used to call it), and both requires to give CPE external IP address, before offers own addresses. It makes a theoretically unsolvable problem. Recently one of Oracle smart guys solved this problem by creating tentative VPN Connection at AWS side, doing what is necessary at Oracle side, going back with Oracle IP address to AWS, and changing VPN Connection Target Customer Gateway. With some limitation works. Find details on this [blog](https://oc-blog.com/2020/03/27/connecting-oci-and-aws-with-native-vpn-services/).

Other method is based on Libreswan, which Oracle recommends to install on compute node at AWS side. There are technical limitation to install Libreswan on Oracle OCI, what was required in my situation, due to operational and contractual constraints. 

This method at start is super easy, as AWS generates [Open|Strong]swan configuration details. If you know Libreswan you will take it and configure your box. If you are not expert, ipsec-partner project is for you. It takes AWS generated files, and does the rest. Initial version however installs and configures single node Libreswan, what is good to start. 

Current version of ipsec-partner integrates Libreswan monitoring with Oracle OCI telemetry, so you may see metrics on OCI console and OCI API. Monitoring integrates anti-flapping algorithm minimizing alerting noise in case of loosing the connection. Offers on-board-diagnostics based on Universal metric collector, with server-less graphical presentation of basic node metics (cpu, memory, network, vnstat, iostat) presented peer day. Integrated chart library delivers rich user experience as zooming.

This installation may be tuned to handle 50MB/s on a minimal commodity instance equipped with 2CPU and network cards. Without this maximum bandwidth is 20MB/s due to CPU saturation by IRQ generated by network card. Notice that Oracle OCI network uses jumbo frames (9000 bytes), but IPsec cuts data down to 1500 bytes, what is MTU size of the Internet. Finally transfers generates huge amount of interrupts, which saturates CPU. Adding one more VNIC and one more CPU load balances network traffic to two processors. 


>To choose the proper version for your environment, you should take a calculator and check what is more economical: (a) native connection w/o HA, (b) Libreswan at AWS, or (c) Libreswan at OCI, and which variant gives you required level of (a) manageability, and (b) fault tolerance.

All of this works well in production system now. The only missing piece was fault tolerance.

# Fault tolerance
Resilience to failures is a complex thing. One should always ask: what may went wrong? The system should cover all possible cases, in practice following Pareto principle i.e. cover 20% of most important situations.

IPsec gateway should be protected against hardware failures at both AWS and OCI side. AWS by design offers two tunnels handled by two IPsec nodes in different fault domains (at leas I read it looking at latency). To change faulted tunnel ipsec-partner offers on this stage manual procedure, however with tunnel state monitoring and anti-flapping algorithm automating routing failover is not complex. To be honest weight route entry most probably is a final solution. Need to think about this. 

The real problem comes with potential OCI node, what is not a typical every day problem, but may happen. A natural choice to handle this situation is standard Linux tool - [pacemaker](https://wiki.clusterlabs.org/wiki/Pacemaker) however when it comes to details thing in the Cloud are not so easy. 

# Bye bye ARP
Cloud infrastructure is highly flexibly, highly integrated piece of engineering. Regular knowledge about networking is not enough when it comes to virtual networks. Years ago it was quite enough to pass IP address between servers, and do the trick with MAC address on a card. Now in modern networks we sill see MAC addresses, and ARP, but it looks both are not playing main role. In Oracle 2 gen OCI instead of MAC address we are dealing with high level identifiers OICD. For example routing is not done to IP level anymore, but OCI routers use IP address' OICD. So: let's say: bye, bye ARP. Welcome OICD.

# Resource managers
Pacemaker interacts with actual elements of services via abstraction level called resource manager. Each single element of the system, which may be used as element of a service is monitored and managed by its resource manager. There is a resource manager for IP address, routing table entry, or Linux service. Resource manager has a simple but absolutely smart (remember KISS?) interface specifying functions and behaviors. Functions are: start, stop, status, monitor, and meta-data, and behavior is defined in spec and may be easily discovered by looking into Dummy resource manager. And guess which technology you need to know to develop resource manager? C, C++, Java, Ruby, python, or perl? Maybe Scala? No. It's just a runnable process (written in any language you like) doing its job and communicating with error levels. By default it's a bash script. 

# OCI resource managers
Unfortunately there is no support in pacemaker for OCI, so it was required to write resource managers to handle floating ip - for both internal interface (exposed as router connecting to AWS networks at the other side of IPsec), and public IP address. 

Both resources at OCI are quire interesting from engineering perspective, as private IP is an information how to configure virtual switches to connect circuits to the right Virtual NIC. The same is with Public IP address, which is not configured at instance's VNIC, but at Internet Gateway level. So finally moving Private IP and Public IP between instances is a matter of associating IP's OICD with VNIC's OICD. Sounds easy, and is easy with help of OCI CLI being a vehicle to super easy use of OCI API.

[oci_private_ip](https://github.com/rstyczynski/ipsec-partner/blob/master/working/oci_privateip.sh) resource manager takes two parameters: private IP address and VNIC number on the instance. In the cluster, the association is done to network card on server selected by pacemaker to be active one, thus conversion to VNIC OICD is done dynamically. Uses OCI methods:
1. oci network vnic assign-private-ip --vnic-id  --ip-address to check if Private IP is already associated to local network card
2. oci network vnic assign-private-ip --vnic-id  --ip-address --unassign-if-already-assigned to forcibly assign Private IP to this instance
3. oci network vnic unassign-private-ip --vnic-id  --ip-address to release Private IP
4. http://169.254.169.254/opc/v1/vnics/ to get VNIC OICD identifier.

Creates info and debug trace at /var/log/oci_privateip.log

[oci_public_ip](https://github.com/rstyczynski/ipsec-partner/blob/master/working/oci_publicip.sh) resource manager takes two parameters as well: public IP address and VNIC number on the instance to connect IP to. Uses OCI methods:
1. oci network public-ip update to assign and release IP
2. oci network public-ip get --public-ip-address to convert IP into OICD,and to check if Public IP is already associated to local network card
3. http://169.254.169.254/opc/v1/vnics/ and oci network private-ip list --vnic-id to get OICD of IP address configured for managed VNIC

Creates info and debug trace at /var/log/oci_publicip.log

Both resource managers works in DEBUG mode now, logging each interaction with OCI platform.

# OCI CLI
OCI resource managers requires access to OCI thus installation and configuration of Oracle OCI is required. Automated installation of Oracle OCI CLI is described here: [README](https://github.com/rstyczynski/ipsec-partner/blob/master/working/README_OCI_CLI.md)

# Libreswan pacemaker cluster
Having resource managers able to handle private and public IP pacemaker may be used to do manage the cluster. The goal is to route traffic directed to AWS networks via functional node, bypassing faulted one. What statutes the fault? I decided to build it out of following rules:
1. Public IP must be on the proper interface
2. Private IP must be on a proper interface
3. Private IP must be associated with a proper interface
3. There must be routing entry to AWS side
4. There must be a ipsec service

At the end above set of rules protects agains failure of the instance. Once one of nodes didn't report status of resources in specified amount of time, other node takes the active role, takes IP addresses, updates routing table, and starts ipsec service.

# Prepare for installation
Before installation of the cluster you need to collect:
1. public IP address
2. private IP address configured in Virtual Subnet Routing Table to direct traffic to AWS subnet
3. local IP address of instances used as cluster members

, on addition to above you need details to configure OCI CLI: 
4. tenancy OCID, user OCID, OCI region name
5. credentials to open OCI console

To install the cluster you need to:
1. install ipsec-partner with actual AWS side configuration files on both nodes of the cluster
2. launch standby instance - it must be exact hardware mirror of the primary one i.e. 2CPU. Linux 7; add secondary VNIC, to guarantee high throughput
3. install OCI CLI
4. both cluster nodes need to be manageable by ansible
5. both nodes must have access to internet to access yum and github

Having above you may configure the cluster. 

# Installation - OCI CLI
OCI CLI installation is described in separated [README](https://github.com/rstyczynski/ipsec-partner/blob/master/working/README_OCI_CLI.md). Complete is before taking next steps.

## Libreswan cluster deployment
Libreswan cluster deployment is automated to maximum extend. The only thing is to ensure you have tool to generate password (which after configuration is not important), and fill configuration files. Files are in yaml and ini formats, which seems to be the best to describe configuration. 

### collect software
Install pwgen - great tool to generate passwords. It's probably Linux now, who requires complex password. pwgen does the job.

```
yum install -y pwgen
```

## specify parameters
Specify cluster parameters. Make sure VNIC numbers nad names are correct.
1. ens3 is the primary interface i.e. no.0
2. ens5 is the secondary interface i.e. no.1
3. routing destination specify as CIDR i.e. network/mask_bits
4. libreswan service name is by default ipsec

```
cat >ipsec_cluster.config <<EOF
---
ipsec_partner:
  cluster:
      nodes:
      - 192.168.6.51
      - 192.168.6.52
      pass: '$(pwgen --symbols --numerals --capitalize --ambiguous 20 | cat)'

  private:
      nic: ens3
      vnic_no: 0
      ip: 192.168.6.251
      cidr_netmask: 24

  public:
      vnic_no: 1
      ip: 132.112.211.16 
  
  route:
    destination: 213.180.141.0/24
    device: ens5
    gateway: 192.168.6.1

  libreswan_service: ipsec
EOF

cat >deploy_ipsec_cluster.inventory <<EOF
[private_ipsec_fra]
192.168.6.51 ansible_user=pmaker ipsec_cluster_role=master
192.168.6.52 ansible_user=pmaker 
EOF
```

## run ansible script
Once all preparation steps are completed, deploy Libreswan cluster. In case of issues verify all parameters; add -vvv to below command line to see detailed debug information.

```
ansible-playbook deploy_ipsec_cluster.yaml -i deploy_ipsec_cluster.inventory
```

Done.



